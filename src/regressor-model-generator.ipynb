{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from pylab import *\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "import loadnotebook\n",
    "from predictionhelper import *\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "import sklearn.metrics as metric\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_config = {6 : [1, 2, 3, 4, 5, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n",
    "                    21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33]}\n",
    "demo_config = {6 : [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 20, \n",
    "                    21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]}\n",
    "sets = [x for x in range(1, 34)]\n",
    "sets.remove(9)\n",
    "sets.remove(12)\n",
    "sets.remove(17)\n",
    "sets.remove(18)\n",
    "sets.remove(19)\n",
    "demo_config = {6 : sets}\n",
    "\n",
    "df_all_data = get_data(config=demo_config, pure=True, refresh=False).reset_index(drop=True)\n",
    "print(len(df_all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_columns = [\"RSRP\", \"RSRQ\", \"SNR\"]\n",
    "pred_index = 2\n",
    "pred_col = prediction_columns[pred_index]\n",
    "group = ['location_x', 'location_y', 'priority', 'set', 'PCI']\n",
    "group2 = ['location_x', 'location_y', 'priority', 'set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = [c for c in df_all_data if \"beam\" in c] + prediction_columns[pred_index+1:]\n",
    "df_data = df_all_data.drop(dropped_columns, axis=1)\n",
    "df_data = df_data[df_data[\"PCI\"].isin(whitelist_PCI)]\n",
    "df_data = df_data.dropna()\n",
    "print(len(df_data))\n",
    "whitelist_PCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data = df_data[group + prediction_columns[:pred_index+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = merge_agg(df_data, group, pred_col, ['count'])\n",
    "new_cols = [x+\"_mean\" for x in prediction_columns[:pred_index+1]]\n",
    "df_data = merge_agg(df_data, group, prediction_columns[:pred_index+1], ['mean'], new_cols)\n",
    "idx = df_data.groupby(group2)['count'].transform(max) == df_data['count']\n",
    "df_data = df_data[idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(pred_index+1) :\n",
    "    df_data[prediction_columns[i]] = df_data[prediction_columns[i] + \"_mean\"]\n",
    "    \n",
    "df_data = df_data.drop(new_cols+['count'], axis=1)\n",
    "df_data = df_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data.groupby(group).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(pred_index+1):\n",
    "#     print(i)\n",
    "#     df_data = merge_agg(df_data, group, \n",
    "#                         prediction_columns[i], ['count'])\n",
    "#     idx = df_data.groupby(group2)['count'].transform(max) == df_data['count']\n",
    "#     df_data = df_data[idx].reset_index(drop=True)\n",
    "#     df_data[prediction_columns[i]] = merge_agg(df_data, group, prediction_columns[i], ['mean'])['mean']\n",
    "#     df_data = df_data.drop_duplicates().reset_index(drop=True)\n",
    "#     df_data = df_data.drop(\"count\", axis=1)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = pd.DataFrame(), pd.DataFrame()\n",
    "data_train_dict, data_test_dict = {}, {}\n",
    "for p in demo_config :\n",
    "    for s in demo_config[p] :\n",
    "        a, b = train_test_split(df_data[df_data.set==s], test_size=0.3, random_state=32)\n",
    "        data_train = data_train.append(a)\n",
    "        data_test = data_test.append(b)   \n",
    "        data_train_dict[(p, s)] = a\n",
    "        data_test_dict[(p, s)] = b\n",
    "print(len(data_train), len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_train_col = ['priority', pred_col]\n",
    "x_train = data_train.drop(exclude_train_col, axis=1)\n",
    "y_train = np.array(data_train[pred_col].values.tolist())\n",
    "x_test = data_test.drop(exclude_train_col, axis=1)\n",
    "y_test = np.array(data_test[pred_col].values.tolist())\n",
    "\n",
    "x_train_dict, y_train_dict, x_test_dict, y_test_dict = {}, {}, {}, {}\n",
    "for p in demo_config :\n",
    "    for s in demo_config[p] :\n",
    "        a, b = data_train_dict[(p,s)], data_test_dict[(p,s)]\n",
    "        x_train_dict[(p, s)] = a.drop(exclude_train_col, axis=1)\n",
    "        y_train_dict[(p, s)] = np.array(a[pred_col].values.tolist())\n",
    "        x_test_dict[(p, s)] = b.drop(exclude_train_col, axis=1)\n",
    "        y_test_dict[(p, s)] = np.array(b[pred_col].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        params = {'kernel':'poly', 'degree':3, 'gamma':'auto', 'coef0':0.0, 'tol':0.001}\n",
    "        params = {'kernel':'rbf', 'degree':3, 'gamma':'auto', 'coef0':0.0, 'tol':0.001}\n",
    "        params['C'] = param['C']\n",
    "        params['epsilon'] = param['epsilon']\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, degree=3, gamma='auto_deprecated', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1):\n",
    "        params = {'degree':degree, 'gamma':gamma, 'coef0':coef0, 'tol':tol, 'C':C, 'epsilon':epsilon}\n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        model =svm.SVR(**params)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        y_pred = model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        mse = metric.mean_squared_error(self.y_test, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        return -1*rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# st = svm_target(x_train, y_train, \n",
    "#                 x_test, y_test)\n",
    "# sBO = BayesianOptimization(st.evaluate, {'C': (0.001, 1), 'epsilon' : (0, 0.1)},\n",
    "#                             random_state = 1)\n",
    "\n",
    "# sBO.maximize(init_points=20, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {'kernel':'rbf'}\n",
    "model = svm.SVR(**svm_params)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "mse = metric.mean_squared_error(y_test, predictions)\n",
    "rmse = math.sqrt(mse)/(max(y_test)-min(y_test))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.weights = ['uniform', 'distance']\n",
    "        self.algorithms = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        params = {'n_neighbors':7}\n",
    "        params['weights'] = self.weights[int(param['weight'])]\n",
    "        params['algorithm'] = self.algorithms[int(param['algorithm'])]\n",
    "        params['leaf_size'] = int(param['leaf_size'])\n",
    "        params['p'] = int(param['p'])\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, weight, algorithm, leaf_size=100, p=2):\n",
    "        params = {}\n",
    "        params['weight'] = weight\n",
    "        params['algorithm'] = algorithm\n",
    "        params['leaf_size'] = int(leaf_size)\n",
    "        params['p'] = int(p)\n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        model = KNeighborsRegressor(**params)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        y_pred = model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        mse = metric.mean_squared_error(self.y_test, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        return -1*rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kt = knn_target(x_train, y_train, \n",
    "#                 x_test, y_test)\n",
    "# kBO = BayesianOptimization(kt.evaluate, {'weight': (0, 1),\n",
    "#                                         'algorithm' : (0, 3),\n",
    "#                                         'leaf_size' : (5, 50),\n",
    "#                                         'p': (1, 2),},\n",
    "#                             random_state = 1)\n",
    "\n",
    "# kBO.maximize(init_points=20, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = kt.clean_param(kBO.res['max']['max_params'])\n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params_list = [\n",
    "{'n_neighbors': len(whitelist_PCI),\n",
    " 'weights': 'distance',\n",
    " 'algorithm': 'brute',\n",
    " 'leaf_size': 5,\n",
    " 'p': 1}, \n",
    "{'n_neighbors': len(whitelist_PCI),\n",
    " 'weights': 'uniform',\n",
    " 'algorithm': 'kd_tree',\n",
    " 'leaf_size': 49,\n",
    " 'p': 1},\n",
    "{'n_neighbors': len(whitelist_PCI),\n",
    " 'weights': 'uniform',\n",
    " 'algorithm': 'kd_tree',\n",
    " 'leaf_size': 9,\n",
    " 'p': 1}\n",
    "]\n",
    "\n",
    "knn_params = knn_params_list[pred_index]\n",
    "# knn_params = {'n_neighbors': 7,\n",
    "#  'weights': 'uniform',\n",
    "#  'algorithm': 'auto',\n",
    "#  'leaf_size': 17,\n",
    "#  'p': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor(**knn_params)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "mse = metric.mean_squared_error(y_test, predictions)\n",
    "rmse = math.sqrt(mse)/(max(y_test)-min(y_test))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xgboost_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        booster_dict = {1:'gbtree', 2:'gblinear', 3:'dart'}\n",
    "        params = {'base_score':0.5, 'booster':'gbtree', 'missing':None, 'n_estimators':100, \n",
    "                  'n_jobs':1, 'random_state':1, 'reg_lambda':1, 'alpha':0, 'scale_pos_weight':1, \n",
    "                  'subsample':1, 'colsample_bytree':1, 'colsample_bylevel':1}\n",
    "        \n",
    "        params['objective'] = 'reg:linear'\n",
    "        params['learning_rate'] = param['learning_rate']/100\n",
    "        params['booster'] = booster_dict[int(param['booster'])]\n",
    "        params['gamma'] = param['gamma']\n",
    "        params['max_depth'] = int(param['max_depth'])\n",
    "        params['min_child_weight'] = int(param['min_child_weight'])\n",
    "        params['max_delta_weight'] = int(param['max_delta_weight'])\n",
    "        params['rate_drop'] = param['rate_drop']\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, learning_rate, booster, gamma, max_depth,  \n",
    "                     min_child_weight, max_delta_weight, rate_drop):\n",
    "\n",
    "        params = {}\n",
    "        params['learning_rate'] = learning_rate\n",
    "        params['booster'] = booster\n",
    "        params['gamma'] = gamma\n",
    "        params['max_depth'] = max_depth\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        params['max_delta_weight'] = max_delta_weight\n",
    "        params['rate_drop'] = rate_drop\n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        xgb_model = XGBRegressor(**params)\n",
    "        xgb_model.fit(self.x_train, self.y_train)\n",
    "        y_pred = xgb_model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        mse = metric.mean_squared_error(self.y_test, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        return -1*rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xt = xgboost_target(x_train, y_train, x_test, y_test)\n",
    "# xgbBO = BayesianOptimization(xt.evaluate, {'learning_rate': (1, 12),\n",
    "#                                             'booster' : (1, 3),\n",
    "#                                             'gamma' : (0, 50),\n",
    "#                                             'max_depth': (3, 12),\n",
    "#                                             'min_child_weight': (1, 1),\n",
    "#                                             'max_delta_weight': (1, 20),\n",
    "#                                             'rate_drop': (0, 1)},\n",
    "#                             random_state = 1)\n",
    "\n",
    "# xgbBO.maximize(init_points=10, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = xt.clean_param(xgbBO.res['max']['max_params'])\n",
    "# xgb_model = XGBRegressor(**params)\n",
    "# xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# y_pred = xgb_model.predict(x_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "# mse = metric.mean_squared_error(y_test, predictions)\n",
    "# rmse = math.sqrt(mse)\n",
    "# print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params_list = [\n",
    "# RSRP,\n",
    "{'base_score': 0.5,\n",
    "'booster': 'gbtree',\n",
    "'missing': None,\n",
    "'n_estimators': 100,\n",
    "'n_jobs': 1,\n",
    "'random_state': 1,\n",
    "'reg_lambda': 1,\n",
    "'alpha': 0,\n",
    "'scale_pos_weight': 1,\n",
    "'subsample': 1,\n",
    "'colsample_bytree': 1,\n",
    "'colsample_bylevel': 1,\n",
    "'objective': 'reg:linear',\n",
    "'learning_rate': 0.06926984074036927,\n",
    "'gamma': 43.90712517147065,\n",
    "'max_depth': 9,\n",
    "'min_child_weight': 1,\n",
    "'max_delta_weight': 14,\n",
    "'rate_drop': 0.5865550405019929},\n",
    "# RSRQ\n",
    "{'base_score': 0.5,\n",
    " 'booster': 'dart',\n",
    " 'missing': None,\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': 1,\n",
    " 'random_state': 1,\n",
    " 'reg_lambda': 1,\n",
    " 'alpha': 0,\n",
    " 'scale_pos_weight': 1,\n",
    " 'subsample': 1,\n",
    " 'colsample_bytree': 1,\n",
    " 'colsample_bylevel': 1,\n",
    " 'objective': 'reg:linear',\n",
    " 'learning_rate': 0.12,\n",
    " 'gamma': 0.0,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 1,\n",
    " 'max_delta_weight': 20,\n",
    " 'rate_drop': 0.0},\n",
    "# SNR\n",
    "{'base_score': 0.5,\n",
    " 'booster': 'dart',\n",
    " 'missing': None,\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': 1,\n",
    " 'random_state': 1,\n",
    " 'reg_lambda': 1,\n",
    " 'alpha': 0,\n",
    " 'scale_pos_weight': 1,\n",
    " 'subsample': 1,\n",
    " 'colsample_bytree': 1,\n",
    " 'colsample_bylevel': 1,\n",
    " 'objective': 'reg:linear',\n",
    " 'learning_rate': 0.12,\n",
    " 'gamma': 0.0,\n",
    " 'max_depth': 12,\n",
    " 'min_child_weight': 1,\n",
    " 'max_delta_weight': 20,\n",
    " 'rate_drop': 0.0}\n",
    "]\n",
    "\n",
    "xgboost_params = xgboost_params_list[pred_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(**xgboost_params)\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "mse = metric.mean_squared_error(y_test, predictions)\n",
    "rmse = math.sqrt(mse)/(max(y_test)-min(y_test))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lgbm_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        params = {'boosting_type':'gbdt', 'class_weight':None, 'colsample_bytree':1.0, \n",
    "                  'importance_type':'split', \n",
    "                  'min_child_samples':20, 'min_split_gain':0.0, 'n_estimators':100, 'objective':None,\n",
    "                  'random_state':0, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n",
    "                  'subsample':1.0, 'subsample_for_bin':200000, 'subsample_freq':0}\n",
    "        params['num_leaves'] = int(param['num_leaves'])\n",
    "        params['min_child_weight'] = int(param['min_child_weight'])\n",
    "        params['max_depth'] = int(param['max_depth'])\n",
    "        params['learning_rate'] = param['learning_rate'] / 100\n",
    "        params['min_data_in_bin'] = 1\n",
    "        params['min_data'] = 1\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, min_child_weight, learning_rate, max_depth, num_leaves):\n",
    "        params = {'num_leaves':num_leaves, \n",
    "                  'min_child_weight':min_child_weight, \n",
    "                  'max_depth':max_depth, \n",
    "                  'learning_rate':learning_rate}\n",
    "        \n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        lgbm_model = LGBMRegressor(**params)\n",
    "        lgbm_model.fit(self.x_train, self.y_train)\n",
    "        y_pred = lgbm_model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        mse = metric.mean_squared_error(self.y_test, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        return -1*rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lt = lgbm_target(x_train, y_train, x_test, y_test)\n",
    "# lgbmBO = BayesianOptimization(lt.evaluate, {'min_child_weight': (0.01, 1),\n",
    "#                                               'learning_rate': (1, 10),\n",
    "#                                               'max_depth': (-1, 15),\n",
    "#                                               'num_leaves': (5, 50)}, \n",
    "#                              random_state=3)\n",
    "\n",
    "# lgbmBO.maximize(init_points=20, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = lt.clean_param(lgbmBO.res['max']['max_params'])\n",
    "# lgbm_model = LGBMRegressor(**params)\n",
    "# lgbm_model.fit(x_train, y_train)\n",
    "# y_pred = lgbm_model.predict(x_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "# mse = metric.mean_squared_error(y_test, predictions)\n",
    "# rmse = math.sqrt(mse)\n",
    "# print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params_list = [\n",
    "# RSRP\n",
    "{'boosting_type': 'gbdt',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'importance_type': 'split',\n",
    " 'min_child_samples': 20,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'objective': None,\n",
    " 'random_state': 0,\n",
    " 'reg_alpha': 0.0,\n",
    " 'reg_lambda': 0.0,\n",
    " 'silent': True,\n",
    " 'subsample': 1.0,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0,\n",
    " 'num_leaves': 50,\n",
    " 'min_child_weight': 0,\n",
    " 'max_depth': -1,\n",
    " 'learning_rate': 0.1,\n",
    " 'min_data_in_bin': 1,\n",
    " 'min_data': 1},\n",
    "# RSRQ\n",
    "{'boosting_type': 'gbdt',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'importance_type': 'split',\n",
    " 'min_child_samples': 20,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'objective': None,\n",
    " 'random_state': 0,\n",
    " 'reg_alpha': 0.0,\n",
    " 'reg_lambda': 0.0,\n",
    " 'silent': True,\n",
    " 'subsample': 1.0,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0,\n",
    " 'num_leaves': 19,\n",
    " 'min_child_weight': 0,\n",
    " 'max_depth': 5,\n",
    " 'learning_rate': 0.0995815310228581,\n",
    " 'min_data_in_bin': 1,\n",
    " 'min_data': 1},\n",
    "# SNR\n",
    "{'boosting_type': 'gbdt',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'importance_type': 'split',\n",
    " 'min_child_samples': 20,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'objective': None,\n",
    " 'random_state': 0,\n",
    " 'reg_alpha': 0.0,\n",
    " 'reg_lambda': 0.0,\n",
    " 'silent': True,\n",
    " 'subsample': 1.0,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0,\n",
    " 'num_leaves': 32,\n",
    " 'min_child_weight': 0,\n",
    " 'max_depth': 11,\n",
    " 'learning_rate': 0.1,\n",
    " 'min_data_in_bin': 1,\n",
    " 'min_data': 1}\n",
    "]\n",
    "\n",
    "lgbm_params = lgbm_params_list[pred_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = LGBMRegressor(**lgbm_params)\n",
    "lgbm_model.fit(x_train, y_train)\n",
    "y_pred = lgbm_model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "mse = metric.mean_squared_error(y_test, predictions)\n",
    "rmse = math.sqrt(mse)/(max(y_test)-min(y_test))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff = np.abs(y_pred - y_test)\n",
    "len(np.where(y_diff<10)[0])/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse_matrix = np.empty((4, 3, 34))\n",
    "nrmse_matrix[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model(model_name, params=None) :\n",
    "    if 'xgb' in model_name :\n",
    "        return XGBRegressor(**xgboost_params) if params is None else XGBRegressor(**params)\n",
    "    elif 'knn' in model_name :\n",
    "        return KNeighborsRegressor(**knn_params) if params is None else KNeighborsRegressor(**params)\n",
    "    elif 'svm' in model_name :\n",
    "        return svm.SVR(**svm_params) if params is None else svm.SVR(**params)\n",
    "    else :\n",
    "        return LGBMRegressor(**lgbm_params) if params is None else LGBMRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_list = ['xgboost', 'lgbm', 'knn', 'svm']\n",
    "model_idx = 0\n",
    "model_name = model_name_list[model_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = reset_model(model_name)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "for s in demo_config[6] :\n",
    "    y_pred = model.predict(x_test_dict[(6, s)])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    mse = metric.mean_squared_error(y_test_dict[(6, s)], predictions)\n",
    "    nrmse = math.sqrt(mse)/(max(y_test_dict[(6, s)]) - min(y_test_dict[(6, s)]))\n",
    "    nrmse_matrix[model_idx, 0, s] = nrmse\n",
    "        \n",
    "pickle.dump(model, open(\"db/%s_%s_baseline.pickle.dat\" % (pred_col, model_name), \"wb\"))\n",
    "for x in nrmse_matrix[model_idx, 0, 1:]:\n",
    "    print('%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in demo_config[6] :\n",
    "    model = reset_model(model_name)\n",
    "    model.fit(x_train_dict[(6, s)], y_train_dict[(6, s)])\n",
    "\n",
    "    y_pred = model.predict(x_test_dict[(6, s)])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    mse = metric.mean_squared_error(y_test_dict[(6, s)], predictions)\n",
    "    nrmse = math.sqrt(mse)/(max(y_test_dict[(6, s)]) - min(y_test_dict[(6, s)]))\n",
    "    nrmse_matrix[model_idx, 1, s] = nrmse\n",
    "        \n",
    "    pickle.dump(model, open(\"db/%s_%s_independent_set_%s.pickle.dat\" % (pred_col, model_name, s), \"wb\"))\n",
    "    \n",
    "for x in nrmse_matrix[model_idx, 1, 1:]:\n",
    "    print('%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in demo_config[6] :\n",
    "    curr_x_train = df_data[df_data.set!=s].drop(exclude_train_col, axis=1)\n",
    "    curr_y_train = df_data[df_data.set!=s][pred_col].values.tolist()\n",
    "    curr_x_test = df_data[df_data.set==s].drop(exclude_train_col, axis=1)\n",
    "    curr_y_test = df_data[df_data.set==s][pred_col].values.tolist()\n",
    "\n",
    "    model = reset_model(model_name)\n",
    "    model.fit(curr_x_train, curr_y_train)\n",
    "\n",
    "    y_pred = model.predict(curr_x_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    mse = metric.mean_squared_error(curr_y_test, predictions)\n",
    "    nrmse = math.sqrt(mse)/(max(curr_y_test) - min(curr_y_test))\n",
    "    nrmse_matrix[model_idx, 2, s] = nrmse\n",
    "\n",
    "    pickle.dump(model, open(\"db/%s_%s_transfer_except_%s.pickle.dat\" % (pred_col, model_name, s), \"wb\"))\n",
    "    \n",
    "for x in nrmse_matrix[model_idx, 2, 1:]:\n",
    "    print('%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = ['baseline', 'independent', 'transfer']\n",
    "for scenario in range(3) :\n",
    "    print(\"========================================\"+scenarios[scenario])\n",
    "    baseline_metrics = nrmse_matrix[:, scenario, 1:]\n",
    "    baseline_metrics = baseline_metrics[~np.isnan(baseline_metrics)].reshape((4,len(sets)))\n",
    "    print('avg of each methods', [ '%.3f' % elem for elem in np.mean(baseline_metrics, axis=1)])\n",
    "    print('best performance', np.unique(np.argmin(baseline_metrics, axis=0), return_counts=True))\n",
    "    \n",
    "    print('diff performance')\n",
    "    min_list = np.min(baseline_metrics, axis=0)\n",
    "\n",
    "    for x in baseline_metrics :\n",
    "        diff = x-min_list\n",
    "        print('%.3f' % np.mean(diff[np.nonzero(diff)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_model in range(4) :\n",
    "    print(\"========================================\"+model_name_list[curr_model])\n",
    "    metrics = nrmse_matrix[curr_model, :, 1:]\n",
    "    metrics = metrics[~np.isnan(metrics)].reshape((3,len(sets)))\n",
    "    print('avg of each scenario', [ '%.3f' % elem for elem in np.mean(metrics, axis=1)])\n",
    "    print('best performance', np.unique(np.argmin(metrics, axis=0), return_counts=True))\n",
    "    \n",
    "    baseline_metrics = nrmse_matrix[curr_model, 0, 1:]\n",
    "    for i in range(1, 3) :\n",
    "        print(scenarios[i])\n",
    "        curr_metrics = nrmse_matrix[curr_model, i, 1:]\n",
    "        drop_idx = curr_metrics > baseline_metrics\n",
    "        diff = baseline_metrics[drop_idx] - curr_metrics[drop_idx]\n",
    "        print('-------decrease %d %d %.3f' % (i, len(diff), np.mean(diff)))\n",
    "        \n",
    "        improve_idx = curr_metrics < baseline_metrics\n",
    "        diff = baseline_metrics[improve_idx] - curr_metrics[improve_idx]\n",
    "        print('+++++++increase  %d %d %.3f' % (i, len(diff), np.mean(diff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predicted Coordinate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cut = 50  \n",
    "y_cut = 100 \n",
    "\n",
    "old_origin_img = cv2.imread('../image/map.png',0)\n",
    "crop = old_origin_img[y_cut:318, x_cut:927]\n",
    "crop = cv2.cvtColor(crop, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "x_coord_list = []\n",
    "y_coord_list = []\n",
    "pci_list = []\n",
    "for lon in range(0, crop.shape[1]) :\n",
    "    for lat in range(0, crop.shape[0]) :\n",
    "        x_coord_list.append(x_cut + lon)\n",
    "        y_coord_list.append(y_cut + lat)\n",
    "        \n",
    "all_x_pci = pd.DataFrame({'location_x':x_coord_list, 'location_y':y_coord_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Opt - Exploration - Partial Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "class target() :\n",
    "    def optimize(self, x, y) :\n",
    "        if self.bayes_opt is None or self.bayes_opt.X is None:\n",
    "            return 1000\n",
    "\n",
    "        bo = self.bayes_opt\n",
    "        bo.gp.fit(bo.X, bo.Y)\n",
    "        mu, sigma = bo.gp.predict(all_x_pci.values, return_std=True)\n",
    "        return -mean(sigma)\n",
    "\n",
    "def posterior(bo, x):\n",
    "    bo.gp.fit(bo.X, bo.Y)\n",
    "    mu, sigma = bo.gp.predict(x, return_std=True)\n",
    "    plot(sigma)\n",
    "    plt.show()\n",
    "    return mu, sigma\n",
    "\n",
    "def plot_gp(bo, x, curr_x_train, curr_y_train, set_val, model, show_sigma_map=False):\n",
    "#     background = get_map_image()\n",
    "#     pci_val = [get_pci(d[0], d[1]) for d in bo.X]\n",
    "#     observation_color = [pci_color_dict[y] if y in pci_color_dict else (255, 255, 255) for y in pci_val]\n",
    "#     path = \"../results/predicted/pci/bayesian_%s_set_%d.png\" % (model, set_val)\n",
    "#     a = visualize(background, bo.X[:, 0].astype(int), bo.X[:, 1].astype(int), \n",
    "#                   observation_color, path, adjustment=True)\n",
    "    \n",
    "    path = \"../results/predicted/pci/real_%s_set_%d.png\" % (model, set_val)\n",
    "    background = get_map_image()\n",
    "    p_color = [pci_decode[y] for y in curr_y_train]\n",
    "    p_color = [pci_color_dict[y] if y in pci_color_dict else (255, 255, 255) for y in p_color]\n",
    "    b = visualize(background, curr_x_train['location_x'].astype(int), curr_x_train['location_y'].astype(int), \n",
    "                  p_color, path, adjustment=True)\n",
    "\n",
    "    if show_sigma_map :\n",
    "        normalize_sigma = matplotlib.colors.Normalize(vmin=min(sigma), vmax=max(sigma))\n",
    "        mu_map = [cmap(normalize_sigma(value))[:3] for value in mu_sigma]\n",
    "        mu_map = [[int(x*255) for x in value] for value in mu_map]    \n",
    "        a=visualize_all_location_heatmap(a, x_coord_view, y_coord_view, mu_map, \n",
    "                                         cmap, normalize_sigma, filename=None,\n",
    "                                         size=1, figsize=(20,10), adjustment=False, show=False)\n",
    "\n",
    "def get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test) :\n",
    "    if 'xgb' in model_name : \n",
    "        return xgboost_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    if 'knn' in model_name : \n",
    "        return knn_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    else : \n",
    "        return lgbm_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    \n",
    "def get_params_range(model_name) :\n",
    "    if 'xgb' in model_name : \n",
    "        return {'learning_rate': (1, 12),\n",
    "                'booster' : (1, 3),\n",
    "                'gamma' : (0, 5),\n",
    "                'max_depth': (3, 10),\n",
    "                'min_child_weight': (1, 1),\n",
    "                'max_delta_weight': (1, 12),\n",
    "                'rate_drop': (0, 1)}\n",
    "    elif 'knn' in model_name :\n",
    "        return {'weight': (0, 1),\n",
    "                'algorithm' : (0, 3),\n",
    "                'leaf_size' : (5, 50),\n",
    "                'p': (1, 2),}\n",
    "    else :\n",
    "        return {'min_child_weight': (0.01, 1),\n",
    "              'learning_rate': (1, 10),\n",
    "              'max_depth': (-1, 15),\n",
    "              'num_leaves': (5, 50)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class target2() :\n",
    "    def __init__(self, set_val) :\n",
    "        self.set_val = set_val\n",
    "    \n",
    "    def optimize(self, x, y) :\n",
    "        if self.bayes_opt is None or self.bayes_opt.X is None or len(self.bayes_opt.X) < 2:\n",
    "            return -1000\n",
    "\n",
    "        curr_df_data = df_data[df_data.set == self.set_val]\n",
    "\n",
    "        temp = curr_df_data.copy()\n",
    "        temp2 = pd.DataFrame(columns=curr_df_data.columns)\n",
    "    \n",
    "        bo = self.bayes_opt\n",
    "        for x in bo.X :\n",
    "            distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "            temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "            temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "        temp3 = curr_df_data[~curr_df_data.index.isin(temp2.index)]\n",
    "        temp2 = curr_df_data[~curr_df_data.index.isin(temp3.index)]\n",
    "\n",
    "        curr_x_train = temp2.drop(exclude_train_col, axis=1)\n",
    "        curr_y_train = np.array(temp2[pred_col].values.tolist())\n",
    "        curr_x_test = temp3.drop(exclude_train_col, axis=1)\n",
    "        curr_y_test = np.array(temp3[pred_col].values.tolist())\n",
    "\n",
    "        params = {'min_child_weight': 0.7151893663724195, 'learning_rate': 4.9382849013642325, \n",
    "                  'max_depth': 7.462318716046472, 'num_leaves': 5.909827884814657,\n",
    "                  'min_data':1, 'min_data_in_bin':1}\n",
    "        t = get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "        params = t.clean_param(params)\n",
    "\n",
    "        model = reset_model(model_name, params)\n",
    "        model.fit(curr_x_train, curr_y_train)\n",
    "\n",
    "        y_pred = model.predict(curr_x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        mse = metric.mean_squared_error(curr_y_test, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        return -1*rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# acc_dict = {}\n",
    "# # for set_val in demo_config[6] :\n",
    "# random = 0\n",
    "# t = target2(2)\n",
    "# bo2 = BayesianOptimization(t.optimize, {'x': (min(x_coord_list), max(x_coord_list)), \n",
    "#                                         'y': (min(y_coord_list), max(y_coord_list))},\n",
    "#                            random_state=random, \n",
    "#                            verbose=1)\n",
    "# t.bayes_opt = bo2\n",
    "\n",
    "# iterations = 50\n",
    "# gp_params = {\"alpha\": 1e-5, \"n_restarts_optimizer\": 3, 'random_state':random}\n",
    "# bo2.maximize(init_points=10, n_iter=iterations, acq=\"ei\", xi=1e+2, **gp_params)\n",
    "# #     bo2.maximize(init_points=2, n_iter=iterations, acq=\"ei\", xi=1e-4, **gp_params)\n",
    "# acc_dict[set_val] = bo2.res['max']['max_val']\n",
    "# print(acc_dict[set_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes_spec_target_inden = np.array([x for x in acc_dict.values()])\n",
    "# for x in list(bayes_spec_target_inden[:, 2]) :\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target : Variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random = 0\n",
    "t = target()\n",
    "bo2 = BayesianOptimization(t.optimize, {'x': (min(x_coord_list), max(x_coord_list)), \n",
    "                                        'y': (min(y_coord_list), max(y_coord_list))},\n",
    "                           random_state=random, \n",
    "                           verbose=1)\n",
    "t.bayes_opt = bo2\n",
    "\n",
    "iterations = 500\n",
    "gp_params = {\"alpha\": 1e-5, \"n_restarts_optimizer\": 3, 'random_state':random}\n",
    "bo2.maximize(init_points=2, n_iter=iterations, acq=\"ei\", xi=1e+2, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Independent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_dict = {}\n",
    "for set_val in demo_config[6] :\n",
    "    curr_df_data = df_data[df_data.set == set_val]\n",
    "    iterations = int(0.5*len(curr_df_data))\n",
    "\n",
    "    temp = curr_df_data.copy()\n",
    "    temp2 = pd.DataFrame(columns=curr_df_data.columns)\n",
    "    \n",
    "#     random = 0\n",
    "#     t = target()\n",
    "#     bo2 = BayesianOptimization(t.optimize, {'x': (min(x_coord_list), max(x_coord_list)), \n",
    "#                                             'y': (min(y_coord_list), max(y_coord_list))},\n",
    "#                                random_state=random, \n",
    "#                                verbose=1)\n",
    "#     t.bayes_opt = bo2\n",
    "#     gp_params = {\"alpha\": 1e-5, \"n_restarts_optimizer\": 3, 'random_state':random}\n",
    "#     bo2.maximize(init_points=2, n_iter=iterations, acq=\"ei\", xi=0.1, **gp_params)\n",
    "#     bo2.maximize(init_points=2, n_iter=iterations, acq=\"poi\", xi=0.1, **gp_params)\n",
    "#     bo2.maximize(init_points=2, n_iter=iterations, acq=\"ucb\", kappa=10, **gp_params)\n",
    "    \n",
    "    for x in bo2.X[:iterations] :\n",
    "        distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "        temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "        temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "#     trouble_col = ['PCI', 'Power_37', 'Power_38', 'Power_39', 'Power_40', 'Power_41', 'Power_42', 'set']\n",
    "#     temp2 = temp2.astype({x:'int' for x in trouble_col})\n",
    "    temp3 = curr_df_data[~curr_df_data.index.isin(temp2.index)]\n",
    "    temp2 = curr_df_data[~curr_df_data.index.isin(temp3.index)]\n",
    "\n",
    "#     curr_x_train = temp2.drop([\"d\"] + exclude_train_col, axis=1)\n",
    "    curr_x_train = temp2.drop(exclude_train_col, axis=1)\n",
    "\n",
    "    curr_y_train = np.array(temp2[pred_col].values.tolist())\n",
    "    curr_x_test = temp3.drop(exclude_train_col, axis=1)\n",
    "    curr_y_test = np.array(temp3[pred_col].values.tolist())\n",
    "\n",
    "    t = get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    bo = BayesianOptimization(t.evaluate, \n",
    "                              get_params_range(model_name),\n",
    "                              random_state = random, \n",
    "                              verbose=0)\n",
    "\n",
    "    bo.maximize(init_points=5, n_iter=1)\n",
    "    params = t.clean_param(bo.res['max']['max_params'])\n",
    "#     params = lgbm_params\n",
    "    \n",
    "    model = reset_model(model_name, params)\n",
    "    model.fit(curr_x_train, curr_y_train)\n",
    "\n",
    "    y_pred = model.predict(curr_x_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    mse = metric.mean_squared_error(curr_y_test, predictions)\n",
    "    rmse = math.sqrt(mse)\n",
    "    print(rmse, bo.res['max']['max_params'])\n",
    "    acc_dict[set_val] = [len(curr_x_train), len(curr_x_test), rmse]\n",
    "    pickle.dump(model, open(\"db/%s_%s_bayesian_independent_%s.pickle.dat\" % \\\n",
    "                            (pred_col, model_name, set_val), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bayes_inden = np.array([x for x in acc_dict.values()])\n",
    "for x in list(bayes_inden[:, 2]) :\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict = {}\n",
    "all_curr_x_train, all_curr_y_train = pd.DataFrame(), []\n",
    "all_curr_x_test_dict, all_curr_y_test_dict = {}, {}\n",
    "for set_val in demo_config[6] :\n",
    "    curr_df_data = df_data[df_data.set == set_val]\n",
    "    iterations = int(0.2*len(curr_df_data))\n",
    "\n",
    "    temp = curr_df_data.copy()\n",
    "    temp2 = pd.DataFrame(columns=curr_df_data.columns)\n",
    "    for x in bo2.X[:iterations] :\n",
    "        distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "        temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "        temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "#     trouble_col = ['PCI', 'Power_37', 'Power_38', 'Power_39', 'Power_40', 'Power_41', 'Power_42', 'set']\n",
    "#     temp2 = temp2.astype({x:'int' for x in trouble_col})\n",
    "    temp3 = curr_df_data[~curr_df_data.index.isin(temp2.index)]\n",
    "    temp2 = curr_df_data[~curr_df_data.index.isin(temp3.index)]\n",
    "\n",
    "#     curr_x_train = temp2.drop([\"d\"] + exclude_train_col, axis=1)\n",
    "    curr_x_train = temp2.drop(exclude_train_col, axis=1)\n",
    "    curr_y_train = temp2[pred_col].values.tolist()\n",
    "    curr_x_test = temp3.drop(exclude_train_col, axis=1)\n",
    "    curr_y_test = temp3[pred_col].values.tolist()\n",
    "    \n",
    "    all_curr_x_train = all_curr_x_train.append(curr_x_train)\n",
    "    all_curr_y_train += curr_y_train \n",
    "    all_curr_x_test_dict[set_val] = curr_x_test\n",
    "    all_curr_y_test_dict[set_val] = curr_y_test  \n",
    "\n",
    "#     plot_gp(bo2, all_x_pci.values, curr_x_train, curr_y_train, set_val, \"xgboost\")\n",
    "    \n",
    "#     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':4.2522, \n",
    "#               'max_delta_weight':11, 'random_state' :random}\n",
    "#     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':1, \n",
    "#               'max_delta_weight':11, 'random_state' :random}\n",
    "\n",
    "t = get_target(model_name, all_curr_x_train, all_curr_y_train, all_curr_x_test, all_curr_y_test)\n",
    "xgbBO = BayesianOptimization(t.evaluate, \n",
    "                             get_params_range(model_name),\n",
    "                             random_state = random, \n",
    "                             verbose=0)\n",
    "\n",
    "xgbBO.maximize(init_points=5, n_iter=3)\n",
    "print(xgbBO.res['max']['max_params'])\n",
    "params = t.clean_param(xgbBO.res['max']['max_params'])\n",
    "\n",
    "# params = lgbm_params\n",
    "# params['min_data_in_bin']=1\n",
    "# params['min_data']=1\n",
    "    \n",
    "model = reset_model(model_name, params)\n",
    "model.fit(curr_x_train, curr_y_train)\n",
    "pickle.dump(model, open(\"db/%s_%s_bayesian_%s.pickle.dat\" % (pred_col, model_name, set_val), \"wb\"))\n",
    "\n",
    "for set_val in demo_config[6] :\n",
    "    y_pred = model.predict(all_curr_x_test_dict[set_val])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    mse = metric.mean_squared_error(all_curr_y_test_dict[set_val], predictions)\n",
    "    rmse = math.sqrt(mse)\n",
    "#     print(rmse)    \n",
    "    acc_dict[set_val] = [len(curr_x_train), len(curr_x_test), rmse]\n",
    "    pickle.dump(model, open(\"db/%s_%s_bayesian_baseline_set_%s.pickle.dat\" % (pred_col, model_name, s), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_baseline = np.array([x for x in acc_dict.values()])\n",
    "for x in list(bayes_baseline[:, 2]) :\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Transfer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_dict = {}\n",
    "all_curr_x_train_dict, all_curr_y_train_dict = {}, {}\n",
    "all_curr_x_test_dict, all_curr_y_test_dict = {}, {}\n",
    "for set_val in demo_config[6] :\n",
    "    curr_df_data = df_data[df_data.set == set_val]\n",
    "    iterations = int(0.2*len(curr_df_data))\n",
    "\n",
    "    temp = curr_df_data.copy()\n",
    "    temp2 = pd.DataFrame(columns=curr_df_data.columns)\n",
    "    for x in bo2.X[:iterations] :\n",
    "        distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "        temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "        temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "#     trouble_col = ['PCI', 'Power_37', 'Power_38', 'Power_39', 'Power_40', 'Power_41', 'Power_42', 'set']\n",
    "#     temp2 = temp2.astype({x:'int' for x in trouble_col})\n",
    "    temp3 = curr_df_data[~curr_df_data.index.isin(temp2.index)]\n",
    "    temp2 = curr_df_data[~curr_df_data.index.isin(temp3.index)]\n",
    "\n",
    "#     curr_x_train = temp2.drop([\"d\"] + exclude_train_col, axis=1)\n",
    "    curr_x_train = temp2.drop(exclude_train_col, axis=1)\n",
    "    curr_y_train = temp2[pred_col].values.tolist()\n",
    "    \n",
    "    all_curr_x_train_dict[set_val] = curr_x_train\n",
    "    all_curr_y_train_dict[set_val] = curr_y_train  \n",
    "    all_curr_x_test_dict[set_val] = curr_df_data.drop(exclude_train_col, axis=1)\n",
    "    all_curr_y_test_dict[set_val] = curr_df_data[pred_col].values.tolist()  \n",
    "\n",
    "#     plot_gp(bo2, all_x_pci.values, curr_x_train, curr_y_train, set_val, \"xgboost\")\n",
    "    \n",
    "#     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':4.2522, \n",
    "#               'max_delta_weight':11, 'random_state' :random}\n",
    "#     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':1, \n",
    "#               'max_delta_weight':11, 'random_state' :random}\n",
    "\n",
    "params = lgbm_params\n",
    "params['min_data_in_bin']=1\n",
    "params['min_data']=1\n",
    "    \n",
    "for set_val in demo_config[6] :\n",
    "    curr_x_train, curr_y_train = pd.DataFrame(), []\n",
    "    for k in all_curr_x_train_dict :\n",
    "        if k != set_val :\n",
    "            curr_x_train = curr_x_train.append(all_curr_x_train_dict[k])\n",
    "            curr_y_train += all_curr_y_train_dict[k]\n",
    "\n",
    "    t = get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    xgbBO = BayesianOptimization(t.evaluate, \n",
    "                                 get_params_range(model_name),\n",
    "                                 random_state = random, \n",
    "                                 verbose=0)\n",
    "\n",
    "    xgbBO.maximize(init_points=5, n_iter=3)\n",
    "    print(xgbBO.res['max']['max_params'])\n",
    "    params = t.clean_param(xgbBO.res['max']['max_params'])\n",
    "\n",
    "    model = reset_model(model_name, params)\n",
    "    model.fit(curr_x_train, curr_y_train)\n",
    "    pickle.dump(model, open(\"db/%s_%s_bayesian_transfer_%s.pickle.dat\" % ('PCI', model_name, set_val), \"wb\"))\n",
    "    \n",
    "    y_pred = model.predict(all_curr_x_test_dict[set_val])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    mse = metric.mean_squared_error(all_curr_y_test_dict[set_val], predictions)\n",
    "    rmse = math.sqrt(mse)\n",
    "    print(rmse)\n",
    "    acc_dict[set_val] = [len(curr_x_train), len(curr_x_test), rmse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_transfer = np.array([x for x in acc_dict.values()])\n",
    "for x in list(bayes_transfer[:, 2]) :\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
